{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl-ixW1DvwBn"
      },
      "source": [
        "# Advanced Calorie Prediction Ensemble Solution\n",
        "\n",
        "This notebook implements an ultra-advanced ensemble solution for calorie prediction with the goal of achieving RMSLE < 0.05.\n",
        "\n",
        "## Structure:\n",
        "1. **Data Loading & Basic Setup**\n",
        "2. **Feature Engineering Functions**\n",
        "3. **Target Encoding Functions**\n",
        "4. **Feature Selection Functions**\n",
        "5. **Ensemble Model Training**\n",
        "6. **Main Execution Pipeline**\n",
        "7. **Results Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LwxzMdYvwBo"
      },
      "source": [
        "##  Imports and Utility Functions\n",
        "Import all required libraries and define the RMSLE scoring function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BuYClaCvwBo"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler, QuantileTransformer, StandardScaler\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def rmsle_score(y_true, y_pred):\n",
        "    \"\"\"Calculate Root Mean Squared Logarithmic Error\"\"\"\n",
        "    y_true = np.maximum(y_true, 1e-8)\n",
        "    y_pred = np.maximum(y_pred, 1e-8)\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsiuJEUYvwBp"
      },
      "source": [
        "##  Data Loading Functions\n",
        "Functions to load and preprocess the training and test datasets with proper data type conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muP5PjOzvwBp"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"Load and preprocess training and test data\"\"\"\n",
        "    train_cols = ['id', 'Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Calories']\n",
        "    test_cols = ['id', 'Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
        "\n",
        "    train = pd.read_csv('train.csv', names=train_cols, skiprows=1)\n",
        "    test = pd.read_csv('test.csv', names=test_cols, skiprows=1)\n",
        "\n",
        "    numeric_cols = ['id', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Calories']\n",
        "    for col in numeric_cols:\n",
        "        if col in train.columns:\n",
        "            train[col] = pd.to_numeric(train[col], errors='coerce')\n",
        "        if col in test.columns and col != 'Calories':\n",
        "            test[col] = pd.to_numeric(test[col], errors='coerce')\n",
        "\n",
        "    print(f\"Training data shape: {train.shape}\")\n",
        "    print(f\"Test data shape: {test.shape}\")\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChsI2iZvwBp"
      },
      "source": [
        "##  Core Feature Engineering\n",
        "Advanced physiological feature engineering including BMI, heart rate zones, body surface area, and metabolic calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvXDGL5vvwBp"
      },
      "outputs": [],
      "source": [
        "def ultra_feature_engineering(df, is_test=False):\n",
        "    \"\"\"Advanced feature engineering with physiological and interaction features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Core physiological features\n",
        "    df['BMI'] = df['Weight'] / ((df['Height']/100) ** 2)\n",
        "    df['Sex_bin'] = (df['Sex'] == 'female').astype(int)\n",
        "    df['Max_HR'] = 220 - df['Age']\n",
        "    df['HR_Intensity'] = df['Heart_Rate'] / df['Max_HR']\n",
        "    df['HR_Reserve'] = df['Max_HR'] - df['Heart_Rate']\n",
        "\n",
        "    # Advanced physiological calculations\n",
        "    df['Body_Surface_Area'] = 0.007184 * (df['Weight'] ** 0.425) * (df['Height'] ** 0.725)\n",
        "    df['Lean_Body_Mass'] = df['Weight'] * (1.10 - 0.0128 * df['BMI'])\n",
        "    df['Basal_Metabolic_Rate'] = np.where(\n",
        "        df['Sex_bin'] == 1,  # Female\n",
        "        655 + (9.6 * df['Weight']) + (1.8 * df['Height']) - (4.7 * df['Age']),\n",
        "        66 + (13.7 * df['Weight']) + (5 * df['Height']) - (6.8 * df['Age'])  # Male\n",
        "    )\n",
        "\n",
        "    # Heart rate zones (more granular)\n",
        "    df['HR_Zone_Fine'] = pd.cut(df['HR_Intensity'],\n",
        "                               bins=[0, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 1.0],\n",
        "                               labels=range(7)).astype(int)\n",
        "\n",
        "    # Advanced interactions\n",
        "    df['Weight_Duration'] = df['Weight'] * df['Duration']\n",
        "    df['HR_Duration'] = df['Heart_Rate'] * df['Duration']\n",
        "    df['Weight_HR'] = df['Weight'] * df['Heart_Rate']\n",
        "    df['BMI_Duration'] = df['BMI'] * df['Duration']\n",
        "    df['BMI_HR'] = df['BMI'] * df['Heart_Rate']\n",
        "    df['Age_HR'] = df['Age'] * df['Heart_Rate']\n",
        "    df['BSA_Duration'] = df['Body_Surface_Area'] * df['Duration']\n",
        "    df['BMR_Duration'] = df['Basal_Metabolic_Rate'] * df['Duration'] / 1440  # Per minute\n",
        "    df['LBM_Duration'] = df['Lean_Body_Mass'] * df['Duration']\n",
        "\n",
        "    # 3-way and 4-way interactions\n",
        "    df['Weight_HR_Duration'] = df['Weight'] * df['Heart_Rate'] * df['Duration']\n",
        "    df['BMI_HR_Duration'] = df['BMI'] * df['Heart_Rate'] * df['Duration']\n",
        "    df['Age_Weight_HR'] = df['Age'] * df['Weight'] * df['Heart_Rate']\n",
        "    df['Intensity_Weight_Duration'] = df['HR_Intensity'] * df['Weight'] * df['Duration']\n",
        "    df['BMR_HR_Duration'] = df['Basal_Metabolic_Rate'] * df['Heart_Rate'] * df['Duration'] / 1440\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpJFcFz0vwBq"
      },
      "source": [
        "##  Specialized Feature Engineering Functions\n",
        "Temperature, metabolic, polynomial, and ratio feature engineering functions for comprehensive feature creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6E5ABxBvwBq"
      },
      "outputs": [],
      "source": [
        "def add_temperature_features(df):\n",
        "    \"\"\"Add temperature-related features\"\"\"\n",
        "    # Temperature features\n",
        "    df['Temp_Diff'] = df['Body_Temp'] - 37.0\n",
        "    df['Temp_Squared'] = df['Temp_Diff'] ** 2\n",
        "    df['Temp_Cubed'] = df['Temp_Diff'] ** 3\n",
        "    df['Temp_Abs'] = np.abs(df['Temp_Diff'])\n",
        "    df['Temp_Category'] = pd.cut(df['Body_Temp'],\n",
        "                                bins=[0, 37.5, 38.5, 39.5, 40.5, 42],\n",
        "                                labels=range(5)).astype(int)\n",
        "    return df\n",
        "\n",
        "def add_metabolic_features(df):\n",
        "    \"\"\"Add advanced metabolic features\"\"\"\n",
        "    # Advanced metabolic features\n",
        "    df['MET_Estimate'] = 3.5 + (df['HR_Intensity'] * 10)  # Metabolic equivalent\n",
        "    df['Calorie_Rate_MET'] = df['MET_Estimate'] * df['Weight'] * df['Duration'] / 60\n",
        "    df['Calorie_Rate_Karvonen'] = df['Weight'] * 0.0175 * df['HR_Intensity'] * df['Duration']\n",
        "    df['Energy_Expenditure'] = df['BMR_Duration'] * (1 + df['HR_Intensity'] * 2)\n",
        "    df['Workout_Efficiency'] = df['HR_Intensity'] / (df['Duration'] / 30)  # Intensity per 30min\n",
        "    return df\n",
        "\n",
        "def add_polynomial_features(df):\n",
        "    \"\"\"Add polynomial features for key variables\"\"\"\n",
        "    # Polynomial features (up to 4th degree for key variables)\n",
        "    for col in ['Duration', 'Weight', 'Heart_Rate', 'BMI', 'HR_Intensity']:\n",
        "        df[f'{col}_Squared'] = df[col] ** 2\n",
        "        df[f'{col}_Cubed'] = df[col] ** 3\n",
        "        if col in ['Duration', 'HR_Intensity']:\n",
        "            df[f'{col}_Fourth'] = df[col] ** 4\n",
        "    return df\n",
        "\n",
        "def add_ratio_features(df):\n",
        "    \"\"\"Add comprehensive ratio features\"\"\"\n",
        "    # Ratio features (comprehensive)\n",
        "    df['Weight_Height_Ratio'] = df['Weight'] / df['Height']\n",
        "    df['Duration_Age_Ratio'] = df['Duration'] / (df['Age'] + 1)\n",
        "    df['HR_Weight_Ratio'] = df['Heart_Rate'] / df['Weight']\n",
        "    df['BMI_Age_Ratio'] = df['BMI'] / (df['Age'] + 1)\n",
        "    df['BSA_Weight_Ratio'] = df['Body_Surface_Area'] / df['Weight']\n",
        "    df['LBM_Weight_Ratio'] = df['Lean_Body_Mass'] / df['Weight']\n",
        "    df['BMR_Weight_Ratio'] = df['Basal_Metabolic_Rate'] / df['Weight']\n",
        "    df['Calorie_per_Minute'] = df['Calorie_Rate_MET'] / (df['Duration'] + 1)\n",
        "    df['Calorie_per_KG'] = df['Calorie_Rate_MET'] / df['Weight']\n",
        "    df['Intensity_per_Age'] = df['HR_Intensity'] / (df['Age'] / 40)  # Normalized by typical age\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnhflZFzvwBq"
      },
      "source": [
        "##  Categorical and Transformation Features\n",
        "Binning, categorical grouping, logarithmic transformations, and complete feature engineering pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjSllMlSvwBq"
      },
      "outputs": [],
      "source": [
        "def add_binning_features(df):\n",
        "    \"\"\"Add binning features with granular categories\"\"\"\n",
        "    # Binning features (very granular)\n",
        "    df['Age_Group'] = pd.cut(df['Age'], bins=np.arange(20, 81, 5), labels=False).fillna(0).astype(int)\n",
        "    df['Weight_Group'] = pd.cut(df['Weight'], bins=np.arange(40, 131, 10), labels=False).fillna(0).astype(int)\n",
        "    df['Duration_Group'] = pd.cut(df['Duration'], bins=np.arange(0, 31, 3), labels=False).fillna(0).astype(int)\n",
        "    df['HR_Group'] = pd.cut(df['Heart_Rate'], bins=np.arange(60, 131, 7), labels=False).fillna(0).astype(int)\n",
        "    df['BMI_Group'] = pd.cut(df['BMI'], bins=[0, 18.5, 22, 25, 28, 30, 35, 100], labels=False).fillna(0).astype(int)\n",
        "\n",
        "    # Interaction between categorical features\n",
        "    df['Age_Weight_Group'] = df['Age_Group'] * 10 + df['Weight_Group']\n",
        "    df['Age_Duration_Group'] = df['Age_Group'] * 10 + df['Duration_Group']\n",
        "    df['Weight_Duration_Group'] = df['Weight_Group'] * 10 + df['Duration_Group']\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_log_sqrt_features(df):\n",
        "    \"\"\"Add log and sqrt transformations\"\"\"\n",
        "    # Log and sqrt transformations\n",
        "    for col in ['Weight', 'Duration', 'Heart_Rate', 'BMI']:\n",
        "        df[f'Log_{col}'] = np.log1p(df[col])\n",
        "        df[f'Sqrt_{col}'] = np.sqrt(df[col])\n",
        "    return df\n",
        "\n",
        "def complete_feature_engineering(df, is_test=False):\n",
        "    \"\"\"Complete feature engineering pipeline with NaN handling\"\"\"\n",
        "    print(f\"Starting feature engineering... Input shape: {df.shape}\")\n",
        "\n",
        "    # Apply all feature engineering steps\n",
        "    df = ultra_feature_engineering(df, is_test)\n",
        "    df = add_temperature_features(df)\n",
        "    df = add_metabolic_features(df)\n",
        "    df = add_polynomial_features(df)\n",
        "    df = add_ratio_features(df)\n",
        "    df = add_binning_features(df)\n",
        "    df = add_log_sqrt_features(df)\n",
        "\n",
        "    # Handle NaN and infinite values\n",
        "    print(\"Checking for NaN and infinite values...\")\n",
        "\n",
        "    # Replace infinite values with NaN first\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Count NaN values\n",
        "    nan_counts = df.isnull().sum()\n",
        "    total_nans = nan_counts.sum()\n",
        "\n",
        "    if total_nans > 0:\n",
        "        print(f\"Found {total_nans} NaN values across {(nan_counts > 0).sum()} columns\")\n",
        "\n",
        "        # Fill NaN values with appropriate strategies\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if df[col].isnull().any():\n",
        "                if col in ['id']:\n",
        "                    continue  # Skip ID column\n",
        "                elif 'Group' in col or 'Category' in col:\n",
        "                    # Categorical features - fill with mode or 0\n",
        "                    df[col] = df[col].fillna(0)\n",
        "                else:\n",
        "                    # Continuous features - fill with median\n",
        "                    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        print(\"NaN values handled successfully!\")\n",
        "\n",
        "    print(f\"Feature engineering complete. Output shape: {df.shape}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzE1ZL3qvwBr"
      },
      "source": [
        "##  Advanced Target Encoding\n",
        "Cross-validated target encoding with multiple statistics (mean, std, median) to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZQ9sh97vwBr"
      },
      "outputs": [],
      "source": [
        "def create_advanced_target_encoding(train_df, test_df, cat_cols, target_col, cv_folds=5):\n",
        "    \"\"\"Create advanced target encoding with multiple statistics\"\"\"\n",
        "    encoded_train = train_df.copy()\n",
        "    encoded_test = test_df.copy()\n",
        "\n",
        "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    print(f\"Creating target encoding for {len(cat_cols)} categorical columns...\")\n",
        "\n",
        "    for col in cat_cols:\n",
        "        # Multiple target statistics\n",
        "        encoded_train[f'{col}_target_mean'] = 0.0\n",
        "        encoded_train[f'{col}_target_std'] = 0.0\n",
        "        encoded_train[f'{col}_target_median'] = 0.0\n",
        "\n",
        "        encoded_test[f'{col}_target_mean'] = 0.0\n",
        "        encoded_test[f'{col}_target_std'] = 0.0\n",
        "        encoded_test[f'{col}_target_median'] = 0.0\n",
        "\n",
        "        # Global statistics\n",
        "        global_mean = train_df[target_col].mean()\n",
        "        global_std = train_df[target_col].std()\n",
        "        global_median = train_df[target_col].median()\n",
        "\n",
        "        # Cross-validation encoding for train\n",
        "        for train_idx, val_idx in kf.split(train_df):\n",
        "            train_fold = train_df.iloc[train_idx]\n",
        "            val_fold = train_df.iloc[val_idx]\n",
        "\n",
        "            # Calculate statistics for each category\n",
        "            target_stats = train_fold.groupby(col)[target_col].agg(['mean', 'std', 'median']).reset_index()\n",
        "            target_stats.columns = [col, 'mean', 'std', 'median']\n",
        "\n",
        "            # Apply to validation fold\n",
        "            val_merged = val_fold[[col]].merge(target_stats, on=col, how='left')\n",
        "            encoded_train.loc[val_idx, f'{col}_target_mean'] = val_merged['mean'].fillna(global_mean)\n",
        "            encoded_train.loc[val_idx, f'{col}_target_std'] = val_merged['std'].fillna(global_std)\n",
        "            encoded_train.loc[val_idx, f'{col}_target_median'] = val_merged['median'].fillna(global_median)\n",
        "\n",
        "        # Encoding for test (using full train data)\n",
        "        target_stats = train_df.groupby(col)[target_col].agg(['mean', 'std', 'median']).reset_index()\n",
        "        target_stats.columns = [col, 'mean', 'std', 'median']\n",
        "\n",
        "        test_merged = test_df[[col]].merge(target_stats, on=col, how='left')\n",
        "        encoded_test[f'{col}_target_mean'] = test_merged['mean'].fillna(global_mean)\n",
        "        encoded_test[f'{col}_target_std'] = test_merged['std'].fillna(global_std)\n",
        "        encoded_test[f'{col}_target_median'] = test_merged['median'].fillna(global_median)\n",
        "\n",
        "    print(\"Target encoding complete!\")\n",
        "    return encoded_train, encoded_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBQI4mhKvwBr"
      },
      "source": [
        "##  Feature Selection\n",
        "Statistical feature selection using F-regression to identify the most important features for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rRRjQLUvwBr"
      },
      "outputs": [],
      "source": [
        "def select_best_features(X, y, n_features=100):\n",
        "    \"\"\"Select best features using statistical tests with NaN handling\"\"\"\n",
        "    print(f\"Selecting top {n_features} features from {X.shape[1]} total features...\")\n",
        "\n",
        "    # Check for NaN values and handle them\n",
        "    if np.isnan(X).any():\n",
        "        print(\"Warning: NaN values detected in features. Applying imputation...\")\n",
        "        from sklearn.impute import SimpleImputer\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        X = imputer.fit_transform(X)\n",
        "        print(\"NaN values handled with median imputation.\")\n",
        "\n",
        "    # Check if we have infinite values\n",
        "    if np.isinf(X).any():\n",
        "        print(\"Warning: Infinite values detected. Replacing with finite values...\")\n",
        "        X = np.where(np.isinf(X), np.finfo(np.float64).max, X)\n",
        "        print(\"Infinite values handled.\")\n",
        "\n",
        "    # Ensure we don't select more features than available\n",
        "    n_features = min(n_features, X.shape[1])\n",
        "\n",
        "    selector = SelectKBest(score_func=f_regression, k=n_features)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "    selected_features = selector.get_support(indices=True)\n",
        "\n",
        "    print(f\"Feature selection complete. Selected {len(selected_features)} features.\")\n",
        "    return X_selected, selected_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0OIkr-jvwBs"
      },
      "source": [
        "##  Level 1 Ensemble Models\n",
        "Definition of diverse Level 1 models including LightGBM, XGBoost, Random Forest, Extra Trees, and Gradient Boosting with optimized hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb4Vy7VBvwBs"
      },
      "outputs": [],
      "source": [
        "def get_level1_models():\n",
        "    \"\"\"Define Level 1 models with optimized hyperparameters\"\"\"\n",
        "    level1_models = {\n",
        "        'lgb_ultra': LGBMRegressor(\n",
        "            n_estimators=2000,\n",
        "            learning_rate=0.01,\n",
        "            num_leaves=127,\n",
        "            max_depth=10,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_alpha=0.001,\n",
        "            reg_lambda=0.001,\n",
        "            min_child_samples=5,\n",
        "            min_child_weight=0.1,\n",
        "            bagging_freq=1,\n",
        "            feature_fraction=0.9,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        ),\n",
        "        'lgb_deep': LGBMRegressor(\n",
        "            n_estimators=1500,\n",
        "            learning_rate=0.015,\n",
        "            num_leaves=255,\n",
        "            max_depth=12,\n",
        "            subsample=0.85,\n",
        "            colsample_bytree=0.85,\n",
        "            reg_alpha=0.005,\n",
        "            reg_lambda=0.005,\n",
        "            min_child_samples=3,\n",
        "            min_child_weight=0.05,\n",
        "            random_state=123,\n",
        "            verbose=-1\n",
        "        ),\n",
        "        'xgb_ultra': XGBRegressor(\n",
        "            n_estimators=1500,\n",
        "            learning_rate=0.01,\n",
        "            max_depth=10,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_alpha=0.001,\n",
        "            reg_lambda=0.001,\n",
        "            min_child_weight=0.1,\n",
        "            gamma=0,\n",
        "            random_state=42,\n",
        "            verbosity=0,\n",
        "            early_stopping_rounds=200\n",
        "        ),\n",
        "        'xgb_deep': XGBRegressor(\n",
        "            n_estimators=1200,\n",
        "            learning_rate=0.015,\n",
        "            max_depth=12,\n",
        "            subsample=0.85,\n",
        "            colsample_bytree=0.85,\n",
        "            reg_alpha=0.005,\n",
        "            reg_lambda=0.005,\n",
        "            min_child_weight=0.05,\n",
        "            gamma=0.1,\n",
        "            random_state=123,\n",
        "            verbosity=0,\n",
        "            early_stopping_rounds=150\n",
        "        ),\n",
        "        'rf_ultra': RandomForestRegressor(\n",
        "            n_estimators=800,\n",
        "            max_depth=15,\n",
        "            min_samples_split=3,\n",
        "            min_samples_leaf=1,\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'et_ultra': ExtraTreesRegressor(\n",
        "            n_estimators=600,\n",
        "            max_depth=12,\n",
        "            min_samples_split=3,\n",
        "            min_samples_leaf=1,\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'gbr': GradientBoostingRegressor(\n",
        "            n_estimators=800,\n",
        "            learning_rate=0.02,\n",
        "            max_depth=8,\n",
        "            subsample=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "    }\n",
        "    return level1_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rl_Kkp3vwBs"
      },
      "source": [
        "##  Level 2 Meta-Learners and Training Pipeline\n",
        "Level 2 meta-learners (Bayesian Ridge, Elastic Net, Ridge, LightGBM) and Level 1 model training with cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQdCCzM7vwBs"
      },
      "outputs": [],
      "source": [
        "def get_level2_models():\n",
        "    \"\"\"Define Level 2 meta-learners\"\"\"\n",
        "    level2_models = {\n",
        "        'bayesian_ridge': BayesianRidge(alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6),\n",
        "        'elastic_net': ElasticNet(alpha=0.5, l1_ratio=0.3, random_state=42),\n",
        "        'ridge': Ridge(alpha=5.0, random_state=42),\n",
        "        'lgb_meta': LGBMRegressor(\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=15,\n",
        "            max_depth=4,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        )\n",
        "    }\n",
        "    return level2_models\n",
        "\n",
        "def train_level1_models(X, y, X_test):\n",
        "    \"\"\"Train Level 1 models and generate out-of-fold predictions\"\"\"\n",
        "    level1_models = get_level1_models()\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Level 1 predictions\n",
        "    level1_train = np.zeros((len(y), len(level1_models)))\n",
        "    level1_test = np.zeros((len(X_test), len(level1_models)))\n",
        "\n",
        "    print(\"Training Level 1 ultra models...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"Fold {fold+1}/5\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        for idx, (name, model) in enumerate(level1_models.items()):\n",
        "            if 'lgb' in name:\n",
        "                model.fit(\n",
        "                    X_train, y_train,\n",
        "                    eval_set=[(X_val, y_val)],\n",
        "                    eval_metric='rmse',\n",
        "                    callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)]\n",
        "                )\n",
        "            elif 'xgb' in name:\n",
        "                model.fit(\n",
        "                    X_train, y_train,\n",
        "                    eval_set=[(X_val, y_val)],\n",
        "                    verbose=False\n",
        "                )\n",
        "            else:  # RF, ET, GBR\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "            # Predictions\n",
        "            val_pred = np.maximum(model.predict(X_val), 0.1)\n",
        "            level1_train[val_idx, idx] = val_pred\n",
        "            level1_test[:, idx] += model.predict(X_test) / 5\n",
        "\n",
        "            val_score = rmsle_score(y_val, val_pred)\n",
        "            print(f\"  {name}: {val_score:.5f}\")\n",
        "\n",
        "    # Ensure positive predictions\n",
        "    level1_test = np.maximum(level1_test, 0.1)\n",
        "\n",
        "    return level1_train, level1_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxg-pSevwBs"
      },
      "source": [
        "##  Ensemble Training and Weighted Combination\n",
        "Level 2 model training, final ensemble creation with performance-based weighting, and complete ultra ensemble pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO_6yXLVvwBt"
      },
      "outputs": [],
      "source": [
        "def train_level2_models(level1_train, y, level1_test):\n",
        "    \"\"\"Train Level 2 meta-learners and create final ensemble\"\"\"\n",
        "    level2_models = get_level2_models()\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    print(\"\\nTraining Level 2 meta-learners...\")\n",
        "\n",
        "    level2_predictions = {}\n",
        "    level2_scores = {}\n",
        "\n",
        "    for name, model in level2_models.items():\n",
        "        oof_pred = np.zeros(len(y))\n",
        "\n",
        "        for train_idx, val_idx in kf.split(level1_train):\n",
        "            X_train_l2, X_val_l2 = level1_train[train_idx], level1_train[val_idx]\n",
        "            y_train_l2, y_val_l2 = y[train_idx], y[val_idx]\n",
        "\n",
        "            if 'lgb' in name:\n",
        "                model.fit(\n",
        "                    X_train_l2, y_train_l2,\n",
        "                    eval_set=[(X_val_l2, y_val_l2)],\n",
        "                    eval_metric='rmse',\n",
        "                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "                )\n",
        "            else:\n",
        "                model.fit(X_train_l2, y_train_l2)\n",
        "\n",
        "            oof_pred[val_idx] = model.predict(X_val_l2)\n",
        "\n",
        "        oof_pred = np.maximum(oof_pred, 0.1)\n",
        "        score = rmsle_score(y, oof_pred)\n",
        "        level2_scores[name] = score\n",
        "\n",
        "        # Train on full data for final prediction\n",
        "        model.fit(level1_train, y)\n",
        "        final_pred = np.maximum(model.predict(level1_test), 0.1)\n",
        "        level2_predictions[name] = final_pred\n",
        "\n",
        "        print(f\"{name}: {score:.5f}\")\n",
        "\n",
        "    return level2_predictions, level2_scores\n",
        "\n",
        "def create_final_ensemble(level2_predictions, level2_scores):\n",
        "    \"\"\"Create weighted ensemble of Level 2 models\"\"\"\n",
        "    # Ensemble of level 2 models (weighted by performance)\n",
        "    scores = list(level2_scores.values())\n",
        "    weights = [1.0 / (score + 1e-8) for score in scores]\n",
        "    weights = [w / sum(weights) for w in weights]\n",
        "\n",
        "    final_prediction = np.zeros(len(next(iter(level2_predictions.values()))))\n",
        "    for (name, pred), weight in zip(level2_predictions.items(), weights):\n",
        "        final_prediction += pred * weight\n",
        "        print(f\"Final weight {name}: {weight:.3f}\")\n",
        "\n",
        "    best_score = min(level2_scores.values())\n",
        "    return final_prediction, best_score\n",
        "\n",
        "def train_ultra_ensemble(X, y, X_test):\n",
        "    \"\"\"Complete ultra ensemble training pipeline\"\"\"\n",
        "    # Train Level 1 models\n",
        "    level1_train, level1_test = train_level1_models(X, y, X_test)\n",
        "\n",
        "    # Train Level 2 models\n",
        "    level2_predictions, level2_scores = train_level2_models(level1_train, y, level1_test)\n",
        "\n",
        "    # Create final ensemble\n",
        "    final_prediction, best_score = create_final_ensemble(level2_predictions, level2_scores)\n",
        "\n",
        "    return final_prediction, best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_hwOso-vwBt"
      },
      "source": [
        "##  Main Execution Pipeline\n",
        "Complete orchestration pipeline that coordinates data loading, feature engineering, target encoding, feature selection, scaling, and ensemble training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SSjAAytvwBt"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ULTRA ADVANCED SUB-0.05 SOLUTION\")\n",
        "    print(\"Target: RMSLE < 0.05\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load data\n",
        "    train, test = load_data()\n",
        "\n",
        "    # Complete feature engineering\n",
        "    print(\"\\nApplying complete feature engineering...\")\n",
        "    train_fe = complete_feature_engineering(train, is_test=False)\n",
        "    test_fe = complete_feature_engineering(test, is_test=True)\n",
        "\n",
        "    # Advanced target encoding\n",
        "    print(\"\\nApplying advanced target encoding...\")\n",
        "    cat_cols = ['Age_Group', 'Weight_Group', 'Duration_Group', 'HR_Group', 'BMI_Group',\n",
        "               'HR_Zone_Fine', 'Temp_Category', 'Age_Weight_Group', 'Age_Duration_Group']\n",
        "    train_fe, test_fe = create_advanced_target_encoding(train_fe, test_fe, cat_cols, 'Calories')\n",
        "\n",
        "    # Prepare features\n",
        "    feature_cols = [col for col in train_fe.columns if col not in ['id', 'Calories', 'Sex']]\n",
        "\n",
        "    X = train_fe[feature_cols].values\n",
        "    y = train_fe['Calories'].values\n",
        "    X_test = test_fe[feature_cols].values\n",
        "\n",
        "    print(f\"\\nTotal features before selection: {len(feature_cols)}\")\n",
        "\n",
        "    # Feature selection\n",
        "    X_selected, selected_indices = select_best_features(X, y, n_features=120)\n",
        "    X_test_selected = X_test[:, selected_indices]\n",
        "\n",
        "    print(f\"Selected features: {len(selected_indices)}\")\n",
        "\n",
        "    # Advanced scaling\n",
        "    print(\"\\nApplying quantile transformation...\")\n",
        "    scaler = QuantileTransformer(n_quantiles=2000, random_state=42)\n",
        "    X_scaled = scaler.fit_transform(X_selected)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "    # Train ultra ensemble\n",
        "    print(\"\\nTraining ultra ensemble...\")\n",
        "    predictions, cv_score = train_ultra_ensemble(X_scaled, y, X_test_scaled)\n",
        "\n",
        "    return train, test, predictions, cv_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqCjtpW5vwBt"
      },
      "source": [
        "## ▶ Execute the Solution\n",
        "Run the complete pipeline to generate predictions and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX6McufivwBu",
        "outputId": "1f423492-ac8f-4286-e6bb-21ea5515c388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ULTRA ADVANCED SUB-0.05 SOLUTION\n",
            "Target: RMSLE < 0.05\n",
            "============================================================\n",
            "Training data shape: (750000, 9)\n",
            "Test data shape: (250000, 8)\n",
            "\n",
            "Applying complete feature engineering...\n",
            "Starting feature engineering... Input shape: (750000, 9)\n",
            "Checking for NaN and infinite values...\n",
            "Feature engineering complete. Output shape: (750000, 80)\n",
            "Starting feature engineering... Input shape: (250000, 8)\n",
            "Checking for NaN and infinite values...\n",
            "Feature engineering complete. Output shape: (250000, 79)\n",
            "\n",
            "Applying advanced target encoding...\n",
            "Creating target encoding for 9 categorical columns...\n",
            "Target encoding complete!\n",
            "\n",
            "Total features before selection: 104\n",
            "Selecting top 120 features from 104 total features...\n",
            "Warning: NaN values detected in features. Applying imputation...\n",
            "NaN values handled with median imputation.\n",
            "Feature selection complete. Selected 104 features.\n",
            "Selected features: 104\n",
            "\n",
            "Applying quantile transformation...\n",
            "\n",
            "Training ultra ensemble...\n",
            "Training Level 1 ultra models...\n",
            "Fold 1/5\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1526]\tvalid_0's rmse: 3.58911\tvalid_0's l2: 12.8817\n",
            "  lgb_ultra: 0.06099\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[547]\tvalid_0's rmse: 3.58755\tvalid_0's l2: 12.8705\n",
            "  lgb_deep: 0.06108\n",
            "  xgb_ultra: 0.06061\n",
            "  xgb_deep: 0.06079\n",
            "  rf_ultra: 0.06111\n",
            "  et_ultra: 0.07137\n",
            "  gbr: 0.06046\n",
            "Fold 2/5\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1817]\tvalid_0's rmse: 3.61818\tvalid_0's l2: 13.0912\n",
            "  lgb_ultra: 0.06163\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[810]\tvalid_0's rmse: 3.61469\tvalid_0's l2: 13.066\n",
            "  lgb_deep: 0.06089\n",
            "  xgb_ultra: 0.06096\n",
            "  xgb_deep: 0.06148\n",
            "  rf_ultra: 0.06238\n",
            "  et_ultra: 0.07302\n"
          ]
        }
      ],
      "source": [
        "# Execute the main pipeline\n",
        "train_data, test_data, predictions, cv_score = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xON92taXvwBu"
      },
      "source": [
        "##  Results Analysis and Submission Creation\n",
        "Create the submission file, display comprehensive results, and analyze model performance against targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nUtca4_vwBu"
      },
      "outputs": [],
      "source": [
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'Calories': predictions\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"ULTRA ADVANCED RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final CV Score: {cv_score:.5f}\")\n",
        "print(f\"Previous best: 0.06305\")\n",
        "print(f\"Target: < 0.05000\")\n",
        "\n",
        "if cv_score < 0.05:\n",
        "    improvement = 0.06305 - cv_score\n",
        "    print(f\"🏆 TARGET ACHIEVED! Improvement: {improvement:.5f}\")\n",
        "elif cv_score < 0.055:\n",
        "    improvement = 0.06305 - cv_score\n",
        "    print(f\"🥇 EXCELLENT! Improvement: {improvement:.5f}\")\n",
        "else:\n",
        "    improvement = 0.06305 - cv_score\n",
        "    print(f\"📈 Progress: {improvement:.5f}\")\n",
        "\n",
        "print(f\"\\nPredictions range: {predictions.min():.2f} to {predictions.max():.2f}\")\n",
        "print(f\"Mean prediction: {predictions.mean():.2f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nSubmission file created: submission.csv\")\n",
        "print(f\"Submission shape: {submission.shape}\")\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2EEv0pHvwBu"
      },
      "source": [
        "##  Visualization and Statistical Analysis\n",
        "Optional detailed analysis with prediction distributions, comparisons with training data, and comprehensive statistical summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h63HGLrvvwBu"
      },
      "outputs": [],
      "source": [
        "# Optional: Additional analysis and visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Prediction distribution analysis\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(predictions, bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.title('Prediction Distribution')\n",
        "plt.xlabel('Predicted Calories')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(train_data['Calories'], bins=50, alpha=0.7, edgecolor='black', label='Train')\n",
        "plt.hist(predictions, bins=50, alpha=0.7, edgecolor='black', label='Predictions')\n",
        "plt.title('Train vs Predictions Distribution')\n",
        "plt.xlabel('Calories')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(range(len(predictions)), predictions, alpha=0.6)\n",
        "plt.title('Prediction Index vs Value')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Predicted Calories')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nPrediction Summary Statistics:\")\n",
        "print(f\"Min: {predictions.min():.2f}\")\n",
        "print(f\"Max: {predictions.max():.2f}\")\n",
        "print(f\"Mean: {predictions.mean():.2f}\")\n",
        "print(f\"Median: {np.median(predictions):.2f}\")\n",
        "print(f\"Std: {predictions.std():.2f}\")\n",
        "\n",
        "print(\"\\nTrain Target Summary Statistics:\")\n",
        "print(f\"Min: {train_data['Calories'].min():.2f}\")\n",
        "print(f\"Max: {train_data['Calories'].max():.2f}\")\n",
        "print(f\"Mean: {train_data['Calories'].mean():.2f}\")\n",
        "print(f\"Median: {train_data['Calories'].median():.2f}\")\n",
        "print(f\"Std: {train_data['Calories'].std():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}